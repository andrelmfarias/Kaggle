{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7d0f74a66080fcb7f36357bae3b8dae5dcd00054",
    "colab_type": "text",
    "id": "1pnaFCv8uD3j"
   },
   "source": [
    "# Challenge Kaggle - Quora Insincere Questions Classification - Simple Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5d7d37b2bc7187160ea643458400afb1da52b9f5",
    "colab_type": "text",
    "id": "VVr5u1YO4IdF"
   },
   "source": [
    "This notebook presents a simple solution for the Kaggle Challenge: [Quora Insincere Questions Classification](https://www.kaggle.com/c/quora-insincere-questions-classification).\n",
    "\n",
    "This problem can be understood as a sentiment analysis problem, one of the most common downstream tasks in NLP.\n",
    "\n",
    "The objective is to build a simple RNN model step-by-step in order to understand how such problem can be tackled.\n",
    "\n",
    "I am going to use the PyTorch library as deep-learning framework.\n",
    "\n",
    "In this solution, I am not going to use one of pre-trained embedding made available by the challenge. In order to present a more didactic solution, I am going to hand-craft the tokenization and we are going to add a embedding layer for training in the model. Please note that this approach will increase the training time, as the model will need to learn the paramaters of the embeddings, which quantity can be huge given the size of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "545a5d254f783d0faeaed201b08951291290f2ee",
    "colab_type": "text",
    "id": "h-the-mXuRy1"
   },
   "source": [
    "## Import of useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "36051c0e5c772fc1ff0321d142ad944fbc33e6bf",
    "colab": {},
    "colab_type": "code",
    "id": "VilPlNhrwCwb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b2beec1b7c8e43d02904d9f6127ab0bd7fcc39e2",
    "colab_type": "text",
    "id": "oENx8XRpvI3g"
   },
   "source": [
    "\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c39435cff1afa467ad2859675a07d956d2a13eef",
    "colab_type": "text",
    "id": "2_4RYe_w36Mf"
   },
   "source": [
    "### Loading data and visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "9f66ced7a4b20eb36528e2ba3ad4872040bd3681",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "lrAEZp29ufAM",
    "outputId": "b4740296-441d-47ee-ce67-407708b4433e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (375806, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "print(\"Train shape : \",train_df.shape)\n",
    "print(\"Test shape : \",test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3d0439e93e0a2e527b0add5d682dae5b1f54ae2",
    "colab_type": "text",
    "id": "DyEkCFDKBk9T"
   },
   "source": [
    "Let's visualizing the structure of the dataframes and the Quora's questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "84d50488ab57831e4f8a7fa00544e1eb1573d7e7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "DeCuml9lBt45",
    "outputId": "b876498a-6cd0-4261-ec72-4c11dc69f11f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  ...   target\n",
       "0  00002165364db923c7e6  ...        0\n",
       "1  000032939017120e6e44  ...        0\n",
       "2  0000412ca6e4628ce2cf  ...        0\n",
       "3  000042bf85aa498cd78e  ...        0\n",
       "4  0000455dfa3e01eae3af  ...        0\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "b05f3274614beb52dacacb6399b4092820fd52ac",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "0MkaVW8EBy4P",
    "outputId": "13a8d3e4-c0ed-46b3-8c33-2e7c3b09eae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How did Quebec nationalists see their province as a nation in the 1960s?\n",
      "Do you have an adopted dog, how would you encourage people to adopt and not shop?\n",
      "Why does velocity affect time? Does velocity affect space geometry?\n",
      "How did Otto von Guericke used the Magdeburg hemispheres?\n",
      "Can I convert montra helicon D to a mountain bike by just changing the tyres?\n"
     ]
    }
   ],
   "source": [
    "for question in train_df.question_text[:5]:\n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f1f77730631569f4eef9b501d32e219b35a361f",
    "colab_type": "text",
    "id": "a6RDhnNlOfha"
   },
   "source": [
    "Some statitistics of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "e792bc8dcefdf3562ab61067ce5d920a4bf9f69f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "N6Lo_GceNq6f",
    "outputId": "52da436e-9c79-4a57-8e12-210c00fb03aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1306122\n",
      "Percentage of insincere questions: 6.19%\n",
      "Number of test samples: 375806\n"
     ]
    }
   ],
   "source": [
    "labels = np.array(train_df.target)\n",
    "\n",
    "print('Number of training samples:', len(train_df))\n",
    "print('Percentage of insincere questions: {:.2%}'.format(labels.sum()/len(train_df)))\n",
    "print('Number of test samples:', len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67e5ece30ab19b20990c2f97df1a93f6b9c29db4"
   },
   "source": [
    "It can be noted that the data set is very unballanced (only 6.19% of the training samples are labeled 1), therefore accuracy is not a good metric to evalutate performance. The metric chosen by the competition, F1-score, is a good metric for evluation of unbalaced datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "93cdd4a94874225a08a6aae073a107eeabcfdc6f",
    "colab_type": "text",
    "id": "hhrrCT-DaHw_"
   },
   "source": [
    "Visualizing a few examples of insincere questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "df1445deb67cfd503cd28cf256701987068b5ccc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "2Kzu0wiyQOSI",
    "outputId": "ab5b76c7-f4f1-4ed6-e663-bae3b469c69d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has the United States become the largest dictatorship in the world?\n",
      "Which babies are more sweeter to their parents? Dark skin babies or light skin babies?\n",
      "If blacks support school choice and mandatory sentencing for criminals why don't they vote Republican?\n",
      "I am gay boy and I love my cousin (boy). He is sexy, but I dont know what to do. He is hot, and I want to see his di**. What should I do?\n",
      "Which races have the smallest penis?\n"
     ]
    }
   ],
   "source": [
    "ins_idx = np.where(np.array(train_df.target)==1)[0]\n",
    "for question in train_df.question_text[ins_idx[:5]]:\n",
    "    print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0a92de91db8c575e5f326c1a5712d3db0d5d247c",
    "colab_type": "text",
    "id": "XoPwK8r_PDuF"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c4d34b29ce40e93075d4daeac2ffa9ba4e6c943b",
    "colab_type": "text",
    "id": "ZzOCWgzAP_-8"
   },
   "source": [
    "### Eliminate punctuation and lower the sentences\n",
    "\n",
    "For this simpler notebook, I am only going to deal with the words in the sentences, without taking into account the punctuation. It is however important to notice that ponctuation might play a big role in this task, as insincere questions are likely to have some particular patterns of punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b5d402d13bdab51b442ba7fee3cb6eaf1cc220c7",
    "colab": {},
    "colab_type": "code",
    "id": "HfSNahd7P-00"
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def lower_eliminate_punctuation(sentence):\n",
    "    '''\n",
    "    Function that takes an string as input, lower it and get rid of its punctuation\n",
    "    '''\n",
    "    filtered_sentence = ''.join([c for c in sentence if c not in punctuation])\n",
    "    return filtered_sentence.lower()\n",
    "\n",
    "# Getting rid of punctuation in both datasets\n",
    "train_df.question_text = train_df.question_text.apply(lower_eliminate_punctuation)\n",
    "test_df.question_text = test_df.question_text.apply(lower_eliminate_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e863dc19a9fcff9e3281c670a52960d8c1ad79a1",
    "colab_type": "text",
    "id": "gK68PyINggsm"
   },
   "source": [
    "### Dealing with outliers\n",
    "\n",
    "If there are empty sentences in the datasets, we should not take them into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "04a85129902a02f9bb4fb1955a4bb2335249f98a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tUrDT69Xgj2l",
    "outputId": "aa47ee2e-4dec-4a42-a9d5-dea6adfc5032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty question at index 420816 with label  1\n"
     ]
    }
   ],
   "source": [
    "# Training set\n",
    "empty_idx_train = []\n",
    "for i, question in enumerate(train_df.question_text):\n",
    "    if question == '':\n",
    "        print('Empty question at index', i, 'with label ', train_df.target[i])\n",
    "        empty_idx_train.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "6f17d9141e4fbad83fa75cf5677624b3eff9c917",
    "colab": {},
    "colab_type": "code",
    "id": "DJfNzQ7qhhaX"
   },
   "outputs": [],
   "source": [
    "# Test set\n",
    "empty_idx_test = []\n",
    "for i, question in enumerate(test_df.question_text):\n",
    "    if question == '':\n",
    "        print('Empty question at index', i, 'with label ', test_df.target[i])\n",
    "        empty_idx_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "95957babdb7167e6396a6c922b9632e1d5aa04d0",
    "colab": {},
    "colab_type": "code",
    "id": "FGvTLOEoihIF"
   },
   "outputs": [],
   "source": [
    "# Eliminating sample with empty question in the training set\n",
    "train_df = train_df.drop(empty_idx_train, axis=0)\n",
    "labels = np.delete(labels, empty_idx_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3bf73c156f02056fd60b67fbb17d9f968c5222e9",
    "colab_type": "text",
    "id": "-Mq3e_9vUT5x"
   },
   "source": [
    "### Creating vocabulary\n",
    "\n",
    "In other to tokenize the sentences, we need to create a vocabulary. I will use a dictionnaire to map word and integer. \n",
    "\n",
    "Note: the first number of the vocabulary will be a 1, as we will use 0 for the padding of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "64502fd0bd480bb261a50d6d3b6719e4100ca44f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "miSqPZ2lWtYN",
    "outputId": "2eaad21d-eb9b-47fc-d255-0865c8544156"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 301219\n"
     ]
    }
   ],
   "source": [
    "# Getting all words in both samples\n",
    "all_text_list = list(train_df.question_text) + list(test_df.question_text)\n",
    "all_text = ' '.join(all_text_list)\n",
    "words = set(all_text.split())\n",
    "\n",
    "print('Number of unique words:', len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "ac767d5bcd4600fa554a53f22723331196257325",
    "colab": {},
    "colab_type": "code",
    "id": "7z4C5yN6XJZD"
   },
   "outputs": [],
   "source": [
    "# Dictionary that maps words to integers\n",
    "word_to_int = {word: i for i, word in enumerate(words, 1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1bd771ea24922c8d1213079e78db89f2581741d1",
    "colab_type": "text",
    "id": "YVuYzSkGXijv"
   },
   "source": [
    "### Tokenizing and padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "506db22a434a7bd321baa64652c0105a80c6263b",
    "colab_type": "text",
    "id": "B_FynwPVYOq5"
   },
   "source": [
    "Tokeninzing questions in both datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "0af44ce32e6614de2f6339b8529ba5c576e96a1b",
    "colab": {},
    "colab_type": "code",
    "id": "j3iLj_hhYNbb"
   },
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    '''\n",
    "    Function that tokenize a sentence using the word_to_int dictionnaire and \n",
    "    return a list of tokens\n",
    "    '''\n",
    "    tokens = []\n",
    "    for word in sentence.split():\n",
    "        tokens.append(word_to_int[word])\n",
    "    return tokens\n",
    "\n",
    "\n",
    "train_tokens = train_df.question_text.apply(tokenize)\n",
    "test_tokens = test_df.question_text.apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4071eb84b2af927e1fabecf727fbbafe68d6693e",
    "colab_type": "text",
    "id": "CKDHZIvIat9W"
   },
   "source": [
    "Checking the size of the longest question in both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "35f4c2e13f47e9b3e91e298cbe84de3fdd83f3c0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "zbx48UvsaNlL",
    "outputId": "55eb7ffc-819a-4694-8290-5c4fef8de5c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of longest question:\n",
      "Training set: 132\n",
      "Test set: 82\n"
     ]
    }
   ],
   "source": [
    "print('Size of longest question:')\n",
    "print('Training set:', max(train_tokens.apply(len)))\n",
    "print('Test set:', max(test_tokens.apply(len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f3b900b7d2aa0f040c48035357c4f28eddfb1cca",
    "colab_type": "text",
    "id": "qiUP2soQbTw9"
   },
   "source": [
    "As the questions are not too long, we can set the sequence length of the samples to the highest value and we will not need to deal with truncating. \n",
    "\n",
    "I will then pad the questions at the left using the token 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "851fb4b47424ed69eb76b934faa56321e474df0c",
    "colab": {},
    "colab_type": "code",
    "id": "m1uQabhramxD"
   },
   "outputs": [],
   "source": [
    "seq_length = max(train_tokens.apply(len)) # == 132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "43719e0892c6abf297cc993821e1f1a70b4d7d62",
    "colab": {},
    "colab_type": "code",
    "id": "8qUaFnnpbukl"
   },
   "outputs": [],
   "source": [
    "def pad(questions, seq_length):\n",
    "    '''\n",
    "    This function pad the questions fed as series of tokens with 0 at left\n",
    "    and returns a numpy array\n",
    "    '''\n",
    "    \n",
    "    features = np.zeros((len(questions), seq_length), dtype=int)\n",
    "    for i, sentence in enumerate(questions):\n",
    "        features[i, -len(sentence):] = sentence\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "374a77ddfd8d57142556387ec15e10fa4d58524d",
    "colab": {},
    "colab_type": "code",
    "id": "SB9Eh0bcdMLk"
   },
   "outputs": [],
   "source": [
    "train = pad(train_tokens, seq_length)\n",
    "test = pad(test_tokens, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8ef6a770c7bc4b6f730a2a3ffc7e4b4bf5a69dd5",
    "colab_type": "text",
    "id": "SoKvgVOmdWNa"
   },
   "source": [
    "### Splitting training data and creating dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7628f531bdfc7f9dacc9f2f92f267d25ce51fc9",
    "colab_type": "text",
    "id": "EYXCGuYGlVam"
   },
   "source": [
    "In order to avoid overfitting, we need to use some data as validation during the training phase.\n",
    "\n",
    "I am going to use a 90/10 ratio for the split of training and validation sets, in order to get a training dataset the closest as possible to the original training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "ac62dcc443c87ee060b40f47db8933363f0452a6",
    "colab": {},
    "colab_type": "code",
    "id": "8TRLy3h3k2CH"
   },
   "outputs": [],
   "source": [
    "x_train, x_val, label_train, label_val = train_test_split(train, labels, test_size=0.1, random_state=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a2380e9ca0021a8ba8913ae3e63d4be3e2cf27bd",
    "colab_type": "text",
    "id": "og2tweaDnhX2"
   },
   "source": [
    "Now, let's create Dataloaders for the datasets that will help us with batch iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "85d5d41590029575b31ecae4e45b3644ae12a403",
    "colab": {},
    "colab_type": "code",
    "id": "vLaQsod_ng5l"
   },
   "outputs": [],
   "source": [
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(label_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(label_val))\n",
    "test_data = TensorDataset(torch.from_numpy(test))\n",
    "\n",
    "# Create Dataloaders\n",
    "batch_size = 56\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df8f2fe14bc39fe58593bba77a2d2db2ab13a6a0",
    "colab_type": "text",
    "id": "H94B9OH7o8ja"
   },
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a24c55389b8ea27bc5ea363e2f21dca1d4a2120",
    "colab_type": "text",
    "id": "12g32tGpo_T3"
   },
   "source": [
    "Now that we have processesed the data, I am going to build a RNN model using 1-Layer GRUs. Such RNN structure is pretty simple and it usually give good results. One can choose to use LSTM instead, I chose GRU because it will have to train less parameters while keeping good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b238b08715ec5e4d01e8a8f9f976c7d8f295d5ff",
    "colab_type": "text",
    "id": "mCuevokFpKXS"
   },
   "source": [
    "### Training on GPU or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "42c9d65e3f2c148b7a560b44f571978b99e3a1bc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pN18m6aioTed",
    "outputId": "6ce2e259-2c99-426d-8f87-97c3b9affedb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# Checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if train_on_gpu:\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "74c23f5d8a5e599c161a47c7667c515ac75c431b",
    "colab_type": "text",
    "id": "v2AJneG-pV7g"
   },
   "source": [
    "### The model\n",
    "\n",
    "The structure of the model is simple:\n",
    "1. Embedding layer. As said above, we are not going to use pre-trained embeddings.\n",
    "2. 1-Layer GRU\n",
    "3. Dropout Layer to avoid overfitting\n",
    "4. Fully connected layer followed by the application of a sigmoid\n",
    "5. Use the output of the last position of the setence as prediction probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "b47409c3e384d49829df8a729279f5ffec81bc33",
    "colab": {},
    "colab_type": "code",
    "id": "yYzc2i7Po7tG"
   },
   "outputs": [],
   "source": [
    "class RNN_model(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used for our classification task\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers\n",
    "        \"\"\"\n",
    "        super(RNN_model, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim       \n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout = drop_prob)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        # Fully-connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "        # Sigmoid layer\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Deal with cases were the current batch_size is different from general batch_size\n",
    "        # It occurrs at the end of iteration with the Dataloaders\n",
    "        if hidden.size(1) != batch_size:\n",
    "            hidden = hidden[:, :batch_size, :].contiguous()\n",
    "        \n",
    "        # Apply embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # GRU Layer\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        \n",
    "        # Stack up GRU outputs --> preparation for the fully-connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # Dropout and fully-connected layers\n",
    "        out = self.dropout(out)\n",
    "        sig_out = self.sigmoid(self.fc(out))\n",
    "        \n",
    "        # Unstack outputs to come back to correct dimensions per sample (batch_size, seq_length)\n",
    "        sig_out = sig_out.contiguous().view(batch_size, -1)\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out[:, -1], hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create a new tensor with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda()\n",
    "            \n",
    "        else:\n",
    "            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f3c0886f91864ada945711abf39d3449a852f2cf",
    "colab_type": "text",
    "id": "OeB7NH2aqWxt"
   },
   "source": [
    "### Defining hyperparameters and initiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "6545cf9c1c481eb807928d20de323e6eb2c6b24c",
    "colab": {},
    "colab_type": "code",
    "id": "2es53fY8qi1C"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_to_int) + 1 # including token 0\n",
    "output_size = 1 # binary classification task \n",
    "embedding_dim = 256\n",
    "hidden_dim = 256\n",
    "n_layers = 1\n",
    "\n",
    "# Initiating the model\n",
    "model = RNN_model(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2b17e139a758cee06d7c3c04fcb09d4f31a64a3e",
    "colab_type": "text",
    "id": "9TZSnvaLq5Dl"
   },
   "source": [
    "## Training\n",
    "\n",
    "We are going to use binary cross-entropy loss (BCELoss()) as loss function and the ADAM optimizer.\n",
    "\n",
    "We are algo going to print the F1-score on valuation set each 1000 steps to follow the progress of the training. Please note that, for simplicity, we are considering a threshold of 0.5 for prediction of a positive label. This is also a hyperparameter that can be learned by cross-validation.\n",
    "\n",
    "We are also going to clip the gradient whenever its norm is higher than 5 in order to avoid the explosion gradient effect that can happen often in RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "48b2dcc1de20373b523520b42c7522af1154c132",
    "colab": {},
    "colab_type": "code",
    "id": "_A2Y4w7brSvt"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "print_every = 1000\n",
    "clip = 5 # gradient clipping - to avoid gradient explosion\n",
    "\n",
    "lr=0.001\n",
    "\n",
    "# Defining loss and optimization functions\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "77aa1d29cccd103ce293a46ab800d2afd906cfc6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "172DNky7rrdJ",
    "outputId": "ad38d3b5-91b4-4026-bd65-972bb8a311bc"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, batch_size, epochs, optimizer, criterion, print_every, clip):\n",
    "    \n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        model.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    # Model in training mode\n",
    "    model.train()\n",
    "    breaker = False\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # Batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            # move data to GPU, if available\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Initialize hidden state\n",
    "            h = model.init_hidden(batch_size)\n",
    "\n",
    "            # Setting accumulated gradients to zero before backward step\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Output from the model\n",
    "            output, _ = model(inputs, h)\n",
    "\n",
    "            # Calculate the loss and perform backprop\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "\n",
    "            # Clipping the gradient to avoid explosion\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            # Backpropagation step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Validation stats\n",
    "            if counter % print_every == 0:\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Get validation loss and F1-score on validation set\n",
    "\n",
    "                    val_losses = []\n",
    "                    all_val_labels = []\n",
    "                    all_val_preds = []\n",
    "\n",
    "                    # Model in evaluation mode\n",
    "                    model.eval()\n",
    "                    for inputs, labels in valid_loader:\n",
    "\n",
    "                        all_val_labels += list(labels)\n",
    "\n",
    "                        # Sending data to GPU\n",
    "                        if(train_on_gpu):\n",
    "                            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                        # Initiating hidden state for the validation set\n",
    "                        val_h = model.init_hidden(batch_size)\n",
    "\n",
    "                        output, _ = model(inputs, val_h)\n",
    "\n",
    "                        # Computing validation loss\n",
    "                        val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                        val_losses.append(val_loss.item())\n",
    "\n",
    "                        # Computing validation F1-score\n",
    "\n",
    "                        preds = torch.round(output.squeeze())  # 1 if output probability >= 0.5\n",
    "                        preds = np.squeeze(preds.numpy()) if not train_on_gpu else np.squeeze(preds.cpu().numpy())\n",
    "                        all_val_preds += list(preds)\n",
    "\n",
    "                current_loss = np.mean(val_losses)\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}...\".format(current_loss),\n",
    "                      \"F1-score: {:.3%}\".format(f1_score(all_val_labels, all_val_preds)))\n",
    "                \n",
    "                # Saving the best model and stopping if there is no improvement after 10 evaluations\n",
    "                \n",
    "                if counter == print_every: # first evaluation\n",
    "                    best_loss = current_loss\n",
    "                    counter_eval = 0  \n",
    "                    \n",
    "                if current_loss < best_loss:\n",
    "                    best_loss = current_loss\n",
    "                    torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "                    counter_eval = 0 \n",
    "                    \n",
    "                counter_eval += 1\n",
    "                if counter_eval == 10:\n",
    "                    breaker = True\n",
    "                    break\n",
    "\n",
    "                # Put model back to training mode\n",
    "                model.train()\n",
    "        \n",
    "        # breaking outer loop on epochs\n",
    "        if breaker:\n",
    "            break\n",
    "    \n",
    "    # Loading best model\n",
    "    state_dict = torch.load('checkpoint.pth')\n",
    "    model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "feb41187955c711f3bc8a8018399e574b39632a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 1000... Loss: 0.040467... Val Loss: 0.140868... F1-score: 45.829%\n",
      "Epoch: 1/4... Step: 2000... Loss: 0.180640... Val Loss: 0.133861... F1-score: 47.592%\n",
      "Epoch: 1/4... Step: 3000... Loss: 0.149161... Val Loss: 0.124398... F1-score: 54.957%\n",
      "Epoch: 1/4... Step: 4000... Loss: 0.091530... Val Loss: 0.121774... F1-score: 53.456%\n",
      "Epoch: 1/4... Step: 5000... Loss: 0.144981... Val Loss: 0.121052... F1-score: 58.301%\n",
      "Epoch: 1/4... Step: 6000... Loss: 0.170608... Val Loss: 0.118826... F1-score: 53.673%\n",
      "Epoch: 1/4... Step: 7000... Loss: 0.070825... Val Loss: 0.117082... F1-score: 52.712%\n",
      "Epoch: 1/4... Step: 8000... Loss: 0.080659... Val Loss: 0.115093... F1-score: 57.109%\n",
      "Epoch: 1/4... Step: 9000... Loss: 0.124474... Val Loss: 0.114922... F1-score: 59.142%\n",
      "Epoch: 1/4... Step: 10000... Loss: 0.033089... Val Loss: 0.115168... F1-score: 55.529%\n",
      "Epoch: 1/4... Step: 11000... Loss: 0.123296... Val Loss: 0.114332... F1-score: 57.940%\n",
      "Epoch: 1/4... Step: 12000... Loss: 0.241941... Val Loss: 0.112573... F1-score: 57.736%\n",
      "Epoch: 1/4... Step: 13000... Loss: 0.040041... Val Loss: 0.112658... F1-score: 56.142%\n",
      "Epoch: 1/4... Step: 14000... Loss: 0.210224... Val Loss: 0.114820... F1-score: 62.269%\n",
      "Epoch: 1/4... Step: 15000... Loss: 0.084681... Val Loss: 0.112034... F1-score: 58.869%\n",
      "Epoch: 1/4... Step: 16000... Loss: 0.126229... Val Loss: 0.111650... F1-score: 58.509%\n",
      "Epoch: 1/4... Step: 17000... Loss: 0.127165... Val Loss: 0.111655... F1-score: 59.854%\n",
      "Epoch: 1/4... Step: 18000... Loss: 0.058184... Val Loss: 0.110884... F1-score: 58.977%\n",
      "Epoch: 1/4... Step: 19000... Loss: 0.108163... Val Loss: 0.111743... F1-score: 61.504%\n",
      "Epoch: 1/4... Step: 20000... Loss: 0.131809... Val Loss: 0.110922... F1-score: 58.900%\n",
      "Epoch: 2/4... Step: 21000... Loss: 0.145154... Val Loss: 0.111913... F1-score: 57.290%\n",
      "Epoch: 2/4... Step: 22000... Loss: 0.073293... Val Loss: 0.112953... F1-score: 59.456%\n",
      "Epoch: 2/4... Step: 23000... Loss: 0.065588... Val Loss: 0.112378... F1-score: 60.614%\n",
      "Epoch: 2/4... Step: 24000... Loss: 0.133197... Val Loss: 0.111253... F1-score: 59.368%\n",
      "Epoch: 2/4... Step: 25000... Loss: 0.143204... Val Loss: 0.112376... F1-score: 55.053%\n",
      "Epoch: 2/4... Step: 26000... Loss: 0.025748... Val Loss: 0.110981... F1-score: 61.060%\n",
      "Epoch: 2/4... Step: 27000... Loss: 0.067271... Val Loss: 0.111512... F1-score: 60.707%\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, valid_loader, batch_size, epochs, optimizer, criterion, print_every, clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b2eba4e5ef794ecd32b121dbf3a7fa8af89878ae",
    "colab_type": "text",
    "id": "Tp-IwnJqtdxW"
   },
   "source": [
    "## Predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "40e99d64d21183b657df46c610933d8e8ad6dda7",
    "colab": {},
    "colab_type": "code",
    "id": "BtbIXn2gggvK"
   },
   "outputs": [],
   "source": [
    "# Model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_test_preds = []\n",
    "\n",
    "    for inputs in test_loader:\n",
    "        inputs = inputs[0]\n",
    "        \n",
    "        # Sending data to GPU\n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "            \n",
    "        test_h = model.init_hidden(batch_size)\n",
    "        output, _ = model(inputs, test_h)\n",
    "        \n",
    "        preds = torch.round(output.squeeze())  # 1 if output probability >= 0.5\n",
    "        preds = np.squeeze(preds.numpy()) if not train_on_gpu else np.squeeze(preds.cpu().numpy())\n",
    "        all_test_preds += list(preds.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "21d1100c117b2fef47f0f9080d3483562f4c6413",
    "colab": {},
    "colab_type": "code",
    "id": "k_wd_dwHhorl"
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({\n",
    "    'qid': test_df.qid,\n",
    "    'prediction': all_test_preds\n",
    "})\n",
    "\n",
    "# Make sure the columns are in the correct order\n",
    "sub = sub[['qid', 'prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "3361d19322e160bdf7c3e6c2fbc81530bcab431a",
    "colab": {},
    "colab_type": "code",
    "id": "0JGmr2_SkNQZ"
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False, sep=',')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ZzY2qDH930Nw"
   ],
   "name": "-Kaggle-Challenge_Quora_Simple.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
