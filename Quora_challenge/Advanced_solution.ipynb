{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5263dc8b35545cfcf180375f42d3e8fe4659674a"
   },
   "source": [
    "# Challenge Kaggle - Quora Insincere Questions Classification - Advanced version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0ac0d08e2ecac89d9d6513fec8583522f405f016"
   },
   "source": [
    "## Introduction\n",
    "This notebook presents a mode advanced solution for the Kaggle Challenge: [Quora Insincere Questions Classification](https://www.kaggle.com/c/quora-insincere-questions-classification).\n",
    "\n",
    "The objective here is to implement and improved solution of the notebook [Simple 1-layer GRU](https://www.kaggle.com/andrelmfarias/simple-1-layer-gru) in order to get a better score.\n",
    "\n",
    "### Improvements and key factors\n",
    "\n",
    "In order to obtain a better model, I implemented some ideas from other kernels / solutions and mixed them with some ideas I had. The main factors of my solution are:\n",
    "1. Use of pre-trained embeddings. It saves much training time, as we do not need to train new embeddings, and it allows us to use intrisec information contained in these embeddings that come from solid models. At the end I decided to use only Glove and Paragram because the vocabularies of the other two models contain less than 50% of the unique words presents in the training and test datasets.\n",
    "2. Use of statistical features such as length of sentence, number of capital letters and number of special characters. This can be usefull because several insincere question presents a special pattern of characters (eg. use of * for bad words). The model use dense layers for these features and then merge with the output from the RNN in order to compute probability of positive target.\n",
    "3. Use of Spacy parser and tokenizer\n",
    "4. Lemmatization and Stemming of words not found in the embeddings vocabulary in order to increase the proportion of known words\n",
    "5. Adaptative length of sequences fed to the model. I used the max length per batch, instead of overall max length, in order to speed up the training.\n",
    "6. Use of bi-directional GRU. It can capture more meaning and context for the sentences than a simple GRU.\n",
    "7. Application of self-attention on the outputs of GRU layer. The self-attention mechanism allows the model to capture better the whole context of the sentence as it send as output a hidden state obtained by a weighted average of all the hidden states generated by the GRU on the sentence.\n",
    "8. Find best threshold for label decision by cross-validation\n",
    "\n",
    "With these improvements, my solution is able to achieve **68%** of F1-score, which is a huge improvement in comparison to the simpler solution (with **60%**).\n",
    "\n",
    "### Other potential improvements\n",
    "\n",
    "Due to the time and hardware constraints, I was not able to perform better cross-validation using 5-fold or 10-fold CV. Indeed, by using only train_test_split as CV technique, our model does not generalize that well as we tend to overfit on the validation set.\n",
    "\n",
    "Another potential improvement would be to train different models with different architectures and/or using the different embeddings, blending the results and the end (Ensemble). It would yield more stable results, as we reduce variance by taking the (weighted) mean of different predictions, and it would probably give us improved results.\n",
    "\n",
    "Trying to do spelling correction might improve the performance as well because it would probabily increase the quantity of known word vectors and yield better results overall.\n",
    "\n",
    "One could also try to train word embeddings using the avalaible corpus (training and test set) in orther to have vector representations for the whole vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "## Import of useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67b9a011d765cca6d98488e8ca631c2409d5ccc6"
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "a35b08d905732c6a28a2ca59166011561a0787a3"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "labels = np.array(train_df.target, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eb5c2a168243fc1bf342e558e0928bee44128ac0"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f73cf3e45a0e93aafd91bd5881e0d689ac45239"
   },
   "source": [
    "### Statistical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "ce72db7cce107b1f381411ad548f8cf7300a46d6"
   },
   "outputs": [],
   "source": [
    "# Functions to extract statistical features\n",
    "\n",
    "def n_upper(sentence):\n",
    "    return len(re.findall(r'[A-Z]',sentence))\n",
    "\n",
    "def n_unique_words(sentence):\n",
    "    return len(set(sentence.split()))\n",
    "\n",
    "def n_question_mark(sentence):\n",
    "    return len(re.findall(r'[?]',sentence))\n",
    "\n",
    "def n_exclamation_mark(sentence):\n",
    "    return len(re.findall(r'[!]',sentence))\n",
    "\n",
    "def n_asterisk(sentence):\n",
    "    return len(re.findall(r'[*]',sentence))\n",
    "\n",
    "def n_parentheses(sentence):\n",
    "    return len(re.findall(r'[()]',sentence))\n",
    "\n",
    "def n_brackets(sentence):\n",
    "    return len(re.findall(r'[\\[\\]]',sentence))\n",
    "\n",
    "def n_braces(sentence):\n",
    "    return len(re.findall(r'[{}]',sentence))\n",
    "\n",
    "def n_quotes(sentence):\n",
    "    return len(re.findall(r'[\"]',sentence))\n",
    "\n",
    "def n_ampersand(sentence):\n",
    "    return len(re.findall(r'[&]',sentence))\n",
    "\n",
    "def n_dash(sentence):\n",
    "    return len(re.findall(r'[-]',sentence))\n",
    "\n",
    "n_stats = 11 # Number of statistical features excluding sequence length\n",
    "\n",
    "def get_stat(questions_list):\n",
    "    ''' \n",
    "    Function that builds matrix of statistical features\n",
    "    '''\n",
    "    stat_feat = np.zeros((len(questions_list), n_stats), dtype=int)\n",
    "    for i,question in tqdm(enumerate(questions_list)):\n",
    "        stat_feat[i,0] = n_upper(question)\n",
    "        stat_feat[i,1] = n_unique_words(question)\n",
    "        stat_feat[i,2] = n_question_mark(question)\n",
    "        stat_feat[i,3] = n_exclamation_mark(question)\n",
    "        stat_feat[i,4] = n_asterisk(question)\n",
    "        stat_feat[i,5] = n_parentheses(question)\n",
    "        stat_feat[i,6] = n_brackets(question)\n",
    "        stat_feat[i,7] = n_braces(question)\n",
    "        stat_feat[i,8] = n_quotes(question)\n",
    "        stat_feat[i,9] = n_ampersand(question)\n",
    "        stat_feat[i,10] = n_dash(question)\n",
    "    \n",
    "    return stat_feat    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "1729841da5b42f1fd962e69116c0c39629782394"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1306122it [00:20, 62939.29it/s]\n",
      "375806it [00:06, 62544.87it/s]\n"
     ]
    }
   ],
   "source": [
    "train_stat = get_stat(train_df.question_text)\n",
    "test_stat = get_stat(test_df.question_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c61fefab36bd274a693b0ff6ba99a39ee7f10dde"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "f09fdf0e79f48644e5ac30f6b4bbb0f85a7ba8fc"
   },
   "outputs": [],
   "source": [
    "# Lowering all the text and storing the lists\n",
    "train_list = list(train_df.question_text.apply(lambda s: s.lower()))\n",
    "test_list = list(test_df.question_text.apply(lambda s: s.lower()))\n",
    "\n",
    "# Getting all text in both samples\n",
    "train_text = ' '.join(train_list)\n",
    "test_text = ' '.join(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "1f140cfeb519cf552f175e7f9fae0b3eb6af01f4"
   },
   "outputs": [],
   "source": [
    "# Using spacy parser and tokenizer\n",
    "nlp = spacy.load(\"en\", disable=['tagger','parser','ner','textcat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "4e80221925960c7c553e471bca94f1bd8d1e25a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1306122it [03:23, 6417.00it/s]\n",
      "375806it [00:56, 6653.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating the vocabulary and tokenizing datasets\n",
    "vocab = {}\n",
    "lemma_vocab = {} # lemmatizing vocabulary\n",
    "word_idx = 1 # start from 1 as we use 0 for padding\n",
    "\n",
    "train_tokens = []\n",
    "for doc in tqdm(nlp.pipe(train_list)):\n",
    "    curr_tokens = []\n",
    "    for token in doc:\n",
    "        if token.text not in vocab:\n",
    "            vocab[token.text] = word_idx\n",
    "            lemma_vocab[token.text] = token.lemma_\n",
    "            word_idx += 1\n",
    "        curr_tokens.append(vocab[token.text])\n",
    "    train_tokens.append(np.array(curr_tokens, dtype=int))\n",
    "\n",
    "test_tokens = []\n",
    "for doc in tqdm(nlp.pipe(test_list)):\n",
    "    curr_tokens = []\n",
    "    for token in doc:\n",
    "        if token.text not in vocab:\n",
    "            vocab[token.text] = word_idx\n",
    "            lemma_vocab[token.text] = token.lemma_\n",
    "            word_idx += 1\n",
    "        curr_tokens.append(vocab[token.text])\n",
    "    test_tokens.append(np.array(curr_tokens, dtype=int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "96055b80cbc00c2ecf21870b8600057ff49112d0"
   },
   "source": [
    "#### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "97ca70df86658c12391f61bba1f42c84df994abc"
   },
   "outputs": [],
   "source": [
    "def pad(questions, seq_length):\n",
    "    '''\n",
    "    This function pad the questions fed as list of tokens with 0 at left\n",
    "    and returns a numpy array, and the length of the sentence at the position 0 of the array.\n",
    "    This length will be useful in order to change the sequences length for each batch during \n",
    "    training and to be used as statistical feature.\n",
    "    '''\n",
    "    \n",
    "    features = np.zeros((len(questions), seq_length+1), dtype=int)\n",
    "    for i, sentence in enumerate(questions):\n",
    "        if len(sentence)==0: # dealing with empty sentences\n",
    "            continue\n",
    "        features[i, 0] = len(sentence)\n",
    "        features[i, -len(sentence):] = sentence\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "34d9da253b6345cc53e508b4bcf0b40ed2e54342"
   },
   "outputs": [],
   "source": [
    "# We are going to use a sequence length that does not truncates any of the samples\n",
    "seq_length = max(max(map(len, train_tokens)), max(map(len, test_tokens))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "c31cf8d3b89aa75b67c119d12eefc83ea52e2d0a"
   },
   "outputs": [],
   "source": [
    "# Applying padding\n",
    "train_tokens = pad(train_tokens, seq_length)\n",
    "test_tokens = pad(test_tokens, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b5af498c999bba4d9b9e0f94023f228a3dfde73e"
   },
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "8d56d26d377f6c67598c7d189b55964039e5dd2c"
   },
   "outputs": [],
   "source": [
    "def get_embeddings(file):\n",
    "    embeddings = {}\n",
    "    with open(file, encoding=\"utf8\", errors='ignore') as f:\n",
    "        for line in tqdm(f):\n",
    "            line_list = line.split(\" \")\n",
    "            if len(line_list) > 100:\n",
    "                embeddings[line_list[0]] = np.array(line_list[1:], dtype='float32')\n",
    "    return embeddings\n",
    "\n",
    "def get_embeddings_matrix(vocab, lemma_vocab, embeddings, keyedVector=False):\n",
    "    \n",
    "    # Stemmers\n",
    "    ps = PorterStemmer()\n",
    "    lc = LancasterStemmer()\n",
    "    sb = SnowballStemmer(\"english\")\n",
    "    \n",
    "    n_words = len(vocab)\n",
    "    if keyedVector:\n",
    "        emb_size = embeddings.vector_size\n",
    "    else:\n",
    "        emb_size = next(iter(embeddings.values())).shape[0]\n",
    "        \n",
    "    # If word2vec, convert it to dict for simplicity and compatibility with the others vectors\n",
    "    if keyedVector:\n",
    "        emb_dict = {}\n",
    "        for word in vocab:\n",
    "            try:\n",
    "                emb_dict[word] = embeddings.get_vector(word)\n",
    "            except:\n",
    "                continue\n",
    "        embeddings = emb_dict\n",
    "    \n",
    "    embedding_matrix = np.zeros((n_words+1, emb_size), dtype=np.float32)\n",
    "    unknown_vec = np.zeros((emb_size,), dtype=np.float32) - 1 # (-1, -1, ..., -1)\n",
    "    unknown_words = 0  # unknown words counter  \n",
    "    for word in tqdm(vocab):\n",
    "        emb_vec = embeddings.get(word)\n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix[vocab[word]] = emb_vec\n",
    "            continue\n",
    "            \n",
    "        # Lemmatizing\n",
    "        emb_vec = embeddings.get(lemma_vocab[word])\n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix[vocab[word]] = emb_vec\n",
    "            continue\n",
    "            \n",
    "        # Stemming\n",
    "        emb_vec = embeddings.get(ps.stem(word))\n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix[vocab[word]] = emb_vec\n",
    "            continue\n",
    "        emb_vec = embeddings.get(lc.stem(word))\n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix[vocab[word]] = emb_vec\n",
    "            continue    \n",
    "        emb_vec = embeddings.get(sb.stem(word))\n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix[vocab[word]] = emb_vec\n",
    "            continue\n",
    "        \n",
    "        # If word vector not found\n",
    "        embedding_matrix[vocab[word]] = unknown_vec\n",
    "        unknown_words += 1\n",
    "        \n",
    "    print('% known words: {:.2%}'.format(1 - unknown_words/n_words))\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5b68aa8cf392d264da175e29f7bfada8f94958f0"
   },
   "source": [
    "#### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "5ab5d88a46d57a746a55c63a32b9eb4609588680"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [03:05, 11812.45it/s]\n",
      "100%|██████████| 254554/254554 [00:06<00:00, 36599.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% known words: 61.21%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting embeddings from file\n",
    "glove_file = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "glove_emb = get_embeddings(glove_file)\n",
    "\n",
    "glove_emb_matrix = get_embeddings_matrix(vocab, lemma_vocab, glove_emb)\n",
    "\n",
    "# Cleaning up memory\n",
    "del glove_emb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3034b3c4de038f6a8be690ef8cf6930db8164707"
   },
   "source": [
    "#### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "06ead262c1c6c4f4753c176bbad6d3f1d889f28d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999995it [01:20, 12497.61it/s]\n",
      "100%|██████████| 254554/254554 [00:08<00:00, 29929.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% known words: 48.61%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting embeddings from file\n",
    "fasttext_file = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "fasttext_emb = get_embeddings(fasttext_file)\n",
    "\n",
    "# Building embedding matrix\n",
    "fasttext_emb_matrix = get_embeddings_matrix(vocab, lemma_vocab, fasttext_emb)\n",
    "\n",
    "# Cleaning up memory\n",
    "del fasttext_emb\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "70ff05358cafa394de4d56e741c7256e9745dd28"
   },
   "source": [
    "#### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "1171657592b8791d8877d50a4ff34ea0482ced73"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254554/254554 [00:09<00:00, 26719.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% known words: 39.11%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting embeddings from file\n",
    "word2vec_file = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "word2vec_emb = KeyedVectors.load_word2vec_format(word2vec_file, binary=True)\n",
    "\n",
    "# Building embedding matrix\n",
    "word2vec_emb_matrix = get_embeddings_matrix(vocab, lemma_vocab, word2vec_emb, keyedVector=True)\n",
    "\n",
    "# Cleaning up memory\n",
    "del word2vec_emb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35beaf6deb63eb4b50d035f4261889db34508e3c"
   },
   "source": [
    "#### Paragram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "a04732273555a278f13093c2d69430493f2dbd4f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1703756it [02:26, 11647.48it/s]\n",
      "100%|██████████| 254554/254554 [00:05<00:00, 45759.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% known words: 70.50%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting embeddings from file\n",
    "paragram_file = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "paragram_emb = get_embeddings(paragram_file)\n",
    "\n",
    "# Building embedding matrix\n",
    "paragram_emb_matrix = get_embeddings_matrix(vocab, lemma_vocab, paragram_emb)\n",
    "\n",
    "# Cleaning up memory\n",
    "del paragram_emb\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22ce3868fe9f3044aa63527eaba400398656213f"
   },
   "source": [
    "#### Concatenating the embedding matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb3c2e6e85a8ce190fba7cb8163721ceedd54ac1"
   },
   "source": [
    "As we can see above, only Glove and Paragram present and acceptable proportion of known words. Therefore, in order to have as many embeddings as possible, I will use only these both as embeddings. I decided to concatenate the embeddings, one can however implement an average or sum of both embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "4fb7f08aec6a105b5d0c231a7ca5b3bc0f3b55b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_matrix = np.concatenate((glove_emb_matrix, \n",
    "                             paragram_emb_matrix), axis=1)\n",
    "\n",
    "del glove_emb_matrix, fasttext_emb_matrix, word2vec_emb_matrix, paragram_emb_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e34e36241d71540ce63e16794c0af88d46f69428"
   },
   "source": [
    "### Splitting training data and creating dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "d0bbf0a0b46f3d3d860fee7c3ea1d5a02698d0d3"
   },
   "outputs": [],
   "source": [
    "# Concatenating tokens and statistical features\n",
    "train_feat = np.concatenate((train_stat, train_tokens), axis=1)\n",
    "test_feat = np.concatenate((test_stat, test_tokens), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "3c2903f342be15d7b857d7507aa78529d33649ba"
   },
   "outputs": [],
   "source": [
    "x_train, x_val, label_train, label_val = train_test_split(train_feat, labels, test_size=0.1, random_state=0) \n",
    "\n",
    "# Create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train), torch.from_numpy(label_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(x_val), torch.from_numpy(label_val))\n",
    "test_data = TensorDataset(torch.from_numpy(test_feat))\n",
    "\n",
    "# Create Dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a661b720b66f98b477fd6376e8f3e96effafb06b"
   },
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "697f53b0f8a61c2fad652e6bcf54d34440f1eb5a"
   },
   "source": [
    "### Training on GPU or CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "ed008d68aed01e1af19feae7adcabd31b08ee2dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# Checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if train_on_gpu:\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0006cfd77debe006cff395b83955225d59b1d380"
   },
   "source": [
    "### The model¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "f4fe834a50dd60fd006d1530e68c520a3dbb3aa8"
   },
   "outputs": [],
   "source": [
    "def init_emb_layer(self, embedding_matrix):\n",
    "    '''\n",
    "    Function to help in the creation of the embedding layer\n",
    "    '''\n",
    "    embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "    num_emb, emb_size = embedding_matrix.size()\n",
    "    emb_layer = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "    return emb_layer\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    '''\n",
    "    Class that implements a Self-Attention module that will be applied on the outputs of the GRU layer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, attention_size, batch_first=False, non_linearity=\"tanh\"):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.attention_weights = nn.Parameter(torch.FloatTensor(attention_size))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        if non_linearity == \"relu\":\n",
    "            self.non_linearity = nn.ReLU()\n",
    "        else:\n",
    "            self.non_linearity = nn.Tanh()\n",
    "\n",
    "        nn.init.uniform(self.attention_weights.data, -0.005, 0.005)\n",
    "\n",
    "    def get_mask(self, attentions, lengths):\n",
    "        \"\"\"\n",
    "        Construct mask for padded itemsteps, based on lengths\n",
    "        \"\"\"\n",
    "        max_len = max(lengths.data)\n",
    "        mask = torch.autograd.Variable(torch.ones(attentions.size())).detach()\n",
    "\n",
    "        if attentions.data.is_cuda:\n",
    "            mask = mask.cuda()\n",
    "\n",
    "        for i, l in enumerate(lengths.data):  # skip the first sentence\n",
    "            if l < max_len:\n",
    "                mask[i, :-l] = 0\n",
    "        return mask\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "\n",
    "        # STEP 1 - perform dot product of the attention vector and each hidden state\n",
    "        \n",
    "        # inputs is a 3D Tensor: batch, len, hidden_size\n",
    "        # scores is a 2D Tensor: batch, len\n",
    "        scores = self.non_linearity(inputs.matmul(self.attention_weights))\n",
    "        scores = self.softmax(scores)\n",
    "\n",
    "        # Step 2 - Masking\n",
    "\n",
    "        # construct a mask, based on the sentence lengths\n",
    "        mask = self.get_mask(scores, lengths)\n",
    "\n",
    "        # apply the mask - zero out masked timesteps\n",
    "        masked_scores = scores * mask\n",
    "\n",
    "        # re-normalize the masked scores\n",
    "        _sums = masked_scores.sum(-1, keepdim=True)  # sums per row\n",
    "        scores = masked_scores.div(_sums)  # divide by row sum\n",
    "\n",
    "        # Step 3 - Weighted sum of hidden states, by the attention scores\n",
    "\n",
    "        # multiply each hidden state with the attention weights\n",
    "        weighted = torch.mul(inputs, scores.unsqueeze(-1).expand_as(inputs))\n",
    "\n",
    "        # sum the hidden states\n",
    "        representations = weighted.sum(1).squeeze()\n",
    "\n",
    "        return representations\n",
    "\n",
    "class Quora_model(nn.Module):\n",
    "    def __init__(self, hidden_layer_dim, embedding_matrix, hidden_dim, gru_layers, stat_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Quora model with bi-directional GRU and self-attention merged with Dense layers\n",
    "        \"\"\"\n",
    "        super(Quora_model, self).__init__()\n",
    "        \n",
    "        self.hidden_layer_dim = hidden_layer_dim\n",
    "        self.gru_layers = gru_layers\n",
    "        self.emb_dim = embedding_matrix.shape[1]\n",
    "        self.hidden_dim = hidden_dim   \n",
    "        self.stat_layers = stat_layers\n",
    "        \n",
    "        # Dense layers for statistical features\n",
    "        stat_in_dim = n_stats + 1 # including sequence length\n",
    "        modules = []\n",
    "        for out_dim in self.stat_layers:\n",
    "            modules.append(nn.Linear(stat_in_dim, out_dim))\n",
    "            modules.append(nn.ReLU())\n",
    "            stat_in_dim = out_dim\n",
    "        \n",
    "        self.stat_dense = nn.Sequential(*modules)\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = init_emb_layer(self, embedding_matrix)\n",
    "        \n",
    "        # Bidirectional GRU layer\n",
    "        self.gru = nn.GRU(self.emb_dim, self.hidden_dim, self.gru_layers, \n",
    "                          batch_first=True, bidirectional=True, dropout = drop_prob)\n",
    "        \n",
    "        # Attention layer\n",
    "        self.attention = SelfAttention(self.hidden_dim*2, batch_first=True)\n",
    "        \n",
    "        # Final dense --- merger of text and statistical features\n",
    "        self.final_dense = nn.Sequential(\n",
    "            nn.Dropout(p=drop_prob),\n",
    "            nn.Linear(self.hidden_dim*2 + out_dim, self.hidden_layer_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=drop_prob),\n",
    "            nn.Linear(self.hidden_layer_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        batch_size, _ = x.size()\n",
    "        \n",
    "        # Deal with cases were the current batch_size is different from general batch_size\n",
    "        # It occurrs at the end of iteration with the Dataloaders\n",
    "        if hidden.size(1) != batch_size:\n",
    "            hidden = hidden[:, :batch_size, :].contiguous()\n",
    "            \n",
    "        # Lengths of sequences\n",
    "        lengths = x[:,n_stats].cpu().numpy().astype(int)\n",
    "        \n",
    "        # Adapting seq_len for the current batch\n",
    "        seq_len = max(lengths) \n",
    "        x_text = x[:, -seq_len:] # input to gru layer\n",
    "        x_stat = x[:, :n_stats+1].type(torch.FloatTensor) # include sequence length as statistical feature\n",
    "        if train_on_gpu:\n",
    "            x_stat = x_stat.cuda()\n",
    "        \n",
    "        # Apply embedding\n",
    "        x_text = self.embedding(x_text)\n",
    "        \n",
    "        # GRU Layer\n",
    "        out_gru, _ = self.gru(x_text, hidden)\n",
    "        \n",
    "        # Apply attention\n",
    "        out_att = self.attention(out_gru, lengths)\n",
    "        \n",
    "        # Dense layer for statistical features\n",
    "        out_stat = self.stat_dense(x_stat)\n",
    "        \n",
    "        # Concatenate output of the RNN with output from statistical features\n",
    "        out = torch.cat((out_att, out_stat), dim=1)\n",
    "        \n",
    "        # Final dense_layer\n",
    "        out = self.final_dense(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create a new tensor with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            hidden = weight.new(self.gru_layers*2, batch_size, self.hidden_dim).zero_().cuda()\n",
    "            \n",
    "        else:\n",
    "            hidden = weight.new(self.gru_layers*2, batch_size, self.hidden_dim).zero_()\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b7f490caa1f6c4c0ec343071e5202452d9b7f54b"
   },
   "source": [
    "### Hyperparameters and model initiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "3fbc4616ae203dde266fd5c8e9592567b28e812e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Quora_model(\n",
       "  (stat_dense): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (embedding): Embedding(254555, 600)\n",
       "  (gru): GRU(600, 256, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  (attention): SelfAttention(\n",
       "    (softmax): Softmax()\n",
       "    (non_linearity): Tanh()\n",
       "  )\n",
       "  (final_dense): Sequential(\n",
       "    (0): Dropout(p=0.1)\n",
       "    (1): Linear(in_features=520, out_features=64, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1)\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim = 256\n",
    "gru_layers = 1\n",
    "dropout = 0.1\n",
    "stat_layers_dim = [16, 8] \n",
    "hidden_layer_dim = 64\n",
    "\n",
    "# Initiating the model\n",
    "model = Quora_model(hidden_layer_dim, emb_matrix, hidden_dim, gru_layers, stat_layers_dim, dropout)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88bc2087d24123d958fc2cbfcd8d905e7ce2eee5"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "2649b99df6d6dbcef1f6118e67435aa568e71339"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "print_every = 1000\n",
    "early_stop = 20\n",
    "clip = 5 # gradient clipping - to avoid gradient explosion\n",
    "\n",
    "lr=0.001\n",
    "\n",
    "# Defining loss and optimization functions\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "605fe8b5b005dd16a5e88427adcc70ded5cc0425"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, batch_size, epochs, \n",
    "                optimizer, criterion, clip, print_every, early_stop):\n",
    "    \n",
    "    # move model to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "        model.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    # Model in training mode\n",
    "    model.train()\n",
    "    breaker = False\n",
    "    for e in range(epochs):\n",
    "\n",
    "        # Batch loop\n",
    "        for inputs, labels in train_loader:\n",
    "            counter += 1\n",
    "\n",
    "            # move data to GPU, if available\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            # Initialize hidden state\n",
    "            h = model.init_hidden(batch_size)\n",
    "\n",
    "            # Setting accumulated gradients to zero before backward step\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Output from the model\n",
    "            output = model(inputs, h)\n",
    "\n",
    "            # Calculate the loss and do backprop step\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "\n",
    "            # Clipping the gradient to avoid explosion\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            # Backpropagation step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Validation stats\n",
    "            if counter % print_every == 0:\n",
    "\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Get validation loss and F1-score on validation set\n",
    "\n",
    "                    val_losses = []\n",
    "                    all_val_labels = []\n",
    "                    all_val_preds = []\n",
    "                    all_val_probs = []\n",
    "\n",
    "                    # Model in evaluation mode\n",
    "                    model.eval()\n",
    "                    for inputs, labels in valid_loader:\n",
    "\n",
    "                        all_val_labels += list(labels)\n",
    "\n",
    "                        # Sending data to GPU\n",
    "                        if(train_on_gpu):\n",
    "                            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                        # Initiating hidden state for the validation set\n",
    "                        val_h = model.init_hidden(batch_size)\n",
    "\n",
    "                        output = model(inputs, val_h)\n",
    "\n",
    "                        # Computing validation loss\n",
    "                        val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                        val_losses.append(val_loss.item())\n",
    "\n",
    "                        # Computing validation F1-score for threshold 0.5\n",
    "\n",
    "                        preds = torch.round(output.squeeze())  # 1 if output probability >= 0.5\n",
    "                        preds = np.squeeze(preds.cpu().numpy())\n",
    "                        all_val_preds += list(preds)\n",
    "                        \n",
    "                        output = np.squeeze(output.cpu().detach().numpy())\n",
    "                        all_val_probs += list(output)\n",
    "\n",
    "                current_loss = np.mean(val_losses)\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.6f}...\".format(current_loss),\n",
    "                      \"F1-score (threshold=0.5): {:.3%}\".format(f1_score(all_val_labels, all_val_preds)))\n",
    "                \n",
    "                # Saving the best model and stopping if there is no improvement after \"early_stop\" evaluations\n",
    "                    \n",
    "                if  counter == print_every or current_loss < best_loss: # first evaluation or improvement\n",
    "                    best_loss = current_loss\n",
    "                    best_val_labels = all_val_labels\n",
    "                    best_probs = all_val_probs\n",
    "                    torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "                    counter_eval = 0 \n",
    "                    \n",
    "                counter_eval += 1\n",
    "                if counter_eval == early_stop:\n",
    "                    breaker = True\n",
    "                    break\n",
    "\n",
    "                # Put model back to training mode\n",
    "                model.train()\n",
    "        \n",
    "        # breaking outer loop on epochs\n",
    "        if breaker:\n",
    "            break\n",
    "    \n",
    "    # Loading best model\n",
    "    state_dict = torch.load('checkpoint.pth')\n",
    "    model.load_state_dict(state_dict)\n",
    "    \n",
    "    return best_probs, best_val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "e56baf82b2683c16b91212ff59e0b94dd01dd680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 1000... Loss: 0.058800... Val Loss: 0.120073... F1-score (threshold=0.5): 51.142%\n",
      "Epoch: 1/4... Step: 2000... Loss: 0.078080... Val Loss: 0.110797... F1-score (threshold=0.5): 57.576%\n",
      "Epoch: 1/4... Step: 3000... Loss: 0.060562... Val Loss: 0.108818... F1-score (threshold=0.5): 60.046%\n",
      "Epoch: 1/4... Step: 4000... Loss: 0.104682... Val Loss: 0.106242... F1-score (threshold=0.5): 57.688%\n",
      "Epoch: 1/4... Step: 5000... Loss: 0.099753... Val Loss: 0.107019... F1-score (threshold=0.5): 64.877%\n",
      "Epoch: 1/4... Step: 6000... Loss: 0.198718... Val Loss: 0.107080... F1-score (threshold=0.5): 60.354%\n",
      "Epoch: 1/4... Step: 7000... Loss: 0.180819... Val Loss: 0.105898... F1-score (threshold=0.5): 63.108%\n",
      "Epoch: 1/4... Step: 8000... Loss: 0.172279... Val Loss: 0.103379... F1-score (threshold=0.5): 60.541%\n",
      "Epoch: 1/4... Step: 9000... Loss: 0.099014... Val Loss: 0.100869... F1-score (threshold=0.5): 63.660%\n",
      "Epoch: 1/4... Step: 10000... Loss: 0.054293... Val Loss: 0.100887... F1-score (threshold=0.5): 65.524%\n",
      "Epoch: 1/4... Step: 11000... Loss: 0.053153... Val Loss: 0.103501... F1-score (threshold=0.5): 67.135%\n",
      "Epoch: 1/4... Step: 12000... Loss: 0.097234... Val Loss: 0.099297... F1-score (threshold=0.5): 64.495%\n",
      "Epoch: 1/4... Step: 13000... Loss: 0.076189... Val Loss: 0.098850... F1-score (threshold=0.5): 63.738%\n",
      "Epoch: 1/4... Step: 14000... Loss: 0.140991... Val Loss: 0.099585... F1-score (threshold=0.5): 66.981%\n",
      "Epoch: 1/4... Step: 15000... Loss: 0.104240... Val Loss: 0.099203... F1-score (threshold=0.5): 64.845%\n",
      "Epoch: 1/4... Step: 16000... Loss: 0.113455... Val Loss: 0.099415... F1-score (threshold=0.5): 61.480%\n",
      "Epoch: 1/4... Step: 17000... Loss: 0.063255... Val Loss: 0.098778... F1-score (threshold=0.5): 63.083%\n",
      "Epoch: 1/4... Step: 18000... Loss: 0.131314... Val Loss: 0.099145... F1-score (threshold=0.5): 64.266%\n",
      "Epoch: 2/4... Step: 19000... Loss: 0.052143... Val Loss: 0.098682... F1-score (threshold=0.5): 66.042%\n",
      "Epoch: 2/4... Step: 20000... Loss: 0.129915... Val Loss: 0.098710... F1-score (threshold=0.5): 67.119%\n",
      "Epoch: 2/4... Step: 21000... Loss: 0.095115... Val Loss: 0.096805... F1-score (threshold=0.5): 66.488%\n",
      "Epoch: 2/4... Step: 22000... Loss: 0.075822... Val Loss: 0.098148... F1-score (threshold=0.5): 66.835%\n",
      "Epoch: 2/4... Step: 23000... Loss: 0.099785... Val Loss: 0.098335... F1-score (threshold=0.5): 66.880%\n",
      "Epoch: 2/4... Step: 24000... Loss: 0.055397... Val Loss: 0.096404... F1-score (threshold=0.5): 66.368%\n",
      "Epoch: 2/4... Step: 25000... Loss: 0.184504... Val Loss: 0.097223... F1-score (threshold=0.5): 66.729%\n",
      "Epoch: 2/4... Step: 26000... Loss: 0.160862... Val Loss: 0.096385... F1-score (threshold=0.5): 66.446%\n",
      "Epoch: 2/4... Step: 27000... Loss: 0.121614... Val Loss: 0.097288... F1-score (threshold=0.5): 66.373%\n",
      "Epoch: 2/4... Step: 28000... Loss: 0.074072... Val Loss: 0.096595... F1-score (threshold=0.5): 66.377%\n",
      "Epoch: 2/4... Step: 29000... Loss: 0.173534... Val Loss: 0.097288... F1-score (threshold=0.5): 66.298%\n",
      "Epoch: 2/4... Step: 30000... Loss: 0.155000... Val Loss: 0.096640... F1-score (threshold=0.5): 66.147%\n",
      "Epoch: 2/4... Step: 31000... Loss: 0.019875... Val Loss: 0.096361... F1-score (threshold=0.5): 67.106%\n",
      "Epoch: 2/4... Step: 32000... Loss: 0.110010... Val Loss: 0.096164... F1-score (threshold=0.5): 66.832%\n",
      "Epoch: 2/4... Step: 33000... Loss: 0.099504... Val Loss: 0.099629... F1-score (threshold=0.5): 67.716%\n",
      "Epoch: 2/4... Step: 34000... Loss: 0.073623... Val Loss: 0.097890... F1-score (threshold=0.5): 66.770%\n",
      "Epoch: 2/4... Step: 35000... Loss: 0.118737... Val Loss: 0.096371... F1-score (threshold=0.5): 65.807%\n",
      "Epoch: 2/4... Step: 36000... Loss: 0.103147... Val Loss: 0.097136... F1-score (threshold=0.5): 65.050%\n",
      "Epoch: 3/4... Step: 37000... Loss: 0.294525... Val Loss: 0.095997... F1-score (threshold=0.5): 64.459%\n",
      "Epoch: 3/4... Step: 38000... Loss: 0.070868... Val Loss: 0.098615... F1-score (threshold=0.5): 67.043%\n",
      "Epoch: 3/4... Step: 39000... Loss: 0.123170... Val Loss: 0.099958... F1-score (threshold=0.5): 67.041%\n",
      "Epoch: 3/4... Step: 40000... Loss: 0.088136... Val Loss: 0.097501... F1-score (threshold=0.5): 67.536%\n",
      "Epoch: 3/4... Step: 41000... Loss: 0.107176... Val Loss: 0.105650... F1-score (threshold=0.5): 65.622%\n",
      "Epoch: 3/4... Step: 42000... Loss: 0.140231... Val Loss: 0.101786... F1-score (threshold=0.5): 67.197%\n",
      "Epoch: 3/4... Step: 43000... Loss: 0.100137... Val Loss: 0.097814... F1-score (threshold=0.5): 64.589%\n",
      "Epoch: 3/4... Step: 44000... Loss: 0.106000... Val Loss: 0.098213... F1-score (threshold=0.5): 64.787%\n",
      "Epoch: 3/4... Step: 45000... Loss: 0.019124... Val Loss: 0.098531... F1-score (threshold=0.5): 67.741%\n",
      "Epoch: 3/4... Step: 46000... Loss: 0.122937... Val Loss: 0.098931... F1-score (threshold=0.5): 67.984%\n",
      "Epoch: 3/4... Step: 47000... Loss: 0.033176... Val Loss: 0.096940... F1-score (threshold=0.5): 67.086%\n",
      "Epoch: 3/4... Step: 48000... Loss: 0.021738... Val Loss: 0.097094... F1-score (threshold=0.5): 67.173%\n",
      "Epoch: 3/4... Step: 49000... Loss: 0.097249... Val Loss: 0.096748... F1-score (threshold=0.5): 67.538%\n",
      "Epoch: 3/4... Step: 50000... Loss: 0.192791... Val Loss: 0.098404... F1-score (threshold=0.5): 66.662%\n",
      "Epoch: 3/4... Step: 51000... Loss: 0.189375... Val Loss: 0.097009... F1-score (threshold=0.5): 67.871%\n",
      "Epoch: 3/4... Step: 52000... Loss: 0.033708... Val Loss: 0.096763... F1-score (threshold=0.5): 66.412%\n",
      "Epoch: 3/4... Step: 53000... Loss: 0.116174... Val Loss: 0.097758... F1-score (threshold=0.5): 67.948%\n",
      "Epoch: 3/4... Step: 54000... Loss: 0.065565... Val Loss: 0.096811... F1-score (threshold=0.5): 67.265%\n",
      "Epoch: 3/4... Step: 55000... Loss: 0.072877... Val Loss: 0.096469... F1-score (threshold=0.5): 67.663%\n",
      "Epoch: 4/4... Step: 56000... Loss: 0.072581... Val Loss: 0.101000... F1-score (threshold=0.5): 67.341%\n",
      "\n",
      "Execution time: 28.47min\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "all_val_probs, all_val_labels = train_model(model, train_loader, valid_loader, batch_size, epochs, \n",
    "                                            optimizer, criterion, clip, print_every, early_stop)\n",
    "tf = time.time()\n",
    "print(\"\\nExecution time: {:.2f}min\".format((tf-t0)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dd61b7987c0660330e6851d868e43e100a6a7681"
   },
   "source": [
    "### Best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "089154b09041012c614990045482b4a972026359"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.000... F1-score 11.446%\n",
      "Threshold: 0.005... F1-score 30.741%\n",
      "Threshold: 0.010... F1-score 37.461%\n",
      "Threshold: 0.015... F1-score 41.705%\n",
      "Threshold: 0.020... F1-score 44.721%\n",
      "Threshold: 0.025... F1-score 47.112%\n",
      "Threshold: 0.030... F1-score 49.012%\n",
      "Threshold: 0.035... F1-score 50.630%\n",
      "Threshold: 0.040... F1-score 52.011%\n",
      "Threshold: 0.045... F1-score 53.247%\n",
      "Threshold: 0.050... F1-score 54.288%\n",
      "Threshold: 0.055... F1-score 55.251%\n",
      "Threshold: 0.060... F1-score 56.138%\n",
      "Threshold: 0.065... F1-score 56.889%\n",
      "Threshold: 0.070... F1-score 57.654%\n",
      "Threshold: 0.075... F1-score 58.366%\n",
      "Threshold: 0.080... F1-score 59.049%\n",
      "Threshold: 0.085... F1-score 59.630%\n",
      "Threshold: 0.090... F1-score 60.157%\n",
      "Threshold: 0.095... F1-score 60.670%\n",
      "Threshold: 0.100... F1-score 61.135%\n",
      "Threshold: 0.105... F1-score 61.609%\n",
      "Threshold: 0.110... F1-score 62.147%\n",
      "Threshold: 0.115... F1-score 62.576%\n",
      "Threshold: 0.120... F1-score 62.900%\n",
      "Threshold: 0.125... F1-score 63.292%\n",
      "Threshold: 0.130... F1-score 63.523%\n",
      "Threshold: 0.135... F1-score 63.889%\n",
      "Threshold: 0.140... F1-score 64.161%\n",
      "Threshold: 0.145... F1-score 64.403%\n",
      "Threshold: 0.150... F1-score 64.730%\n",
      "Threshold: 0.155... F1-score 65.000%\n",
      "Threshold: 0.160... F1-score 65.257%\n",
      "Threshold: 0.165... F1-score 65.450%\n",
      "Threshold: 0.170... F1-score 65.639%\n",
      "Threshold: 0.175... F1-score 65.910%\n",
      "Threshold: 0.180... F1-score 66.143%\n",
      "Threshold: 0.185... F1-score 66.322%\n",
      "Threshold: 0.190... F1-score 66.483%\n",
      "Threshold: 0.195... F1-score 66.656%\n",
      "Threshold: 0.200... F1-score 66.787%\n",
      "Threshold: 0.205... F1-score 66.844%\n",
      "Threshold: 0.210... F1-score 66.995%\n",
      "Threshold: 0.215... F1-score 67.187%\n",
      "Threshold: 0.220... F1-score 67.317%\n",
      "Threshold: 0.225... F1-score 67.504%\n",
      "Threshold: 0.230... F1-score 67.609%\n",
      "Threshold: 0.235... F1-score 67.690%\n",
      "Threshold: 0.240... F1-score 67.808%\n",
      "Threshold: 0.245... F1-score 67.763%\n",
      "Threshold: 0.250... F1-score 67.824%\n",
      "Threshold: 0.255... F1-score 67.896%\n",
      "Threshold: 0.260... F1-score 67.996%\n",
      "Threshold: 0.265... F1-score 67.969%\n",
      "Threshold: 0.270... F1-score 68.084%\n",
      "Threshold: 0.275... F1-score 68.178%\n",
      "Threshold: 0.280... F1-score 68.170%\n",
      "Threshold: 0.285... F1-score 68.292%\n",
      "Threshold: 0.290... F1-score 68.354%\n",
      "Threshold: 0.295... F1-score 68.416%\n",
      "Threshold: 0.300... F1-score 68.424%\n",
      "Threshold: 0.305... F1-score 68.456%\n",
      "Threshold: 0.310... F1-score 68.446%\n",
      "Threshold: 0.315... F1-score 68.401%\n",
      "Threshold: 0.320... F1-score 68.452%\n",
      "Threshold: 0.325... F1-score 68.572%\n",
      "Threshold: 0.330... F1-score 68.598%\n",
      "Threshold: 0.335... F1-score 68.672%\n",
      "Threshold: 0.340... F1-score 68.611%\n",
      "Threshold: 0.345... F1-score 68.580%\n",
      "Threshold: 0.350... F1-score 68.535%\n",
      "Threshold: 0.355... F1-score 68.514%\n",
      "Threshold: 0.360... F1-score 68.450%\n",
      "Threshold: 0.365... F1-score 68.461%\n",
      "Threshold: 0.370... F1-score 68.500%\n",
      "Threshold: 0.375... F1-score 68.433%\n",
      "Threshold: 0.380... F1-score 68.388%\n",
      "Threshold: 0.385... F1-score 68.292%\n",
      "Threshold: 0.390... F1-score 68.162%\n",
      "Threshold: 0.395... F1-score 68.055%\n",
      "Threshold: 0.400... F1-score 67.977%\n",
      "Threshold: 0.405... F1-score 67.912%\n",
      "Threshold: 0.410... F1-score 67.881%\n",
      "Threshold: 0.415... F1-score 67.696%\n",
      "Threshold: 0.420... F1-score 67.526%\n",
      "Threshold: 0.425... F1-score 67.364%\n",
      "Threshold: 0.430... F1-score 67.226%\n",
      "Threshold: 0.435... F1-score 67.057%\n",
      "Threshold: 0.440... F1-score 67.006%\n",
      "Threshold: 0.445... F1-score 66.814%\n",
      "Threshold: 0.450... F1-score 66.599%\n",
      "Threshold: 0.455... F1-score 66.540%\n",
      "Threshold: 0.460... F1-score 66.375%\n",
      "Threshold: 0.465... F1-score 66.282%\n",
      "Threshold: 0.470... F1-score 65.989%\n",
      "Threshold: 0.475... F1-score 65.790%\n",
      "Threshold: 0.480... F1-score 65.542%\n",
      "Threshold: 0.485... F1-score 65.226%\n",
      "Threshold: 0.490... F1-score 65.045%\n",
      "Threshold: 0.495... F1-score 64.746%\n",
      "\n",
      "Best threshold: 0.335... F1-score 68.672%\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "for thr in np.arange(0.0, 0.5, 0.005):\n",
    "    pred = np.array(all_val_probs > thr, dtype=int)\n",
    "    score = f1_score(all_val_labels, pred)\n",
    "    print(\"Threshold: {:.3f}... F1-score {:.3%}\".format(thr, score))\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_thr = thr\n",
    "print(\"\\nBest threshold: {:.3f}... F1-score {:.3%}\".format(best_thr, best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "da3093bdfce8690c1619affcf33b896ebc8e7054"
   },
   "source": [
    "## Predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "0f8265bbc979bd10bcde2ac9c6ce3dbfee96508b"
   },
   "outputs": [],
   "source": [
    "# Model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_test_preds = []\n",
    "\n",
    "    for inputs in test_loader:\n",
    "        inputs = inputs[0]\n",
    "        \n",
    "        # Sending data to GPU\n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "            \n",
    "        test_h = model.init_hidden(batch_size)\n",
    "        \n",
    "        output = model(inputs, test_h)\n",
    "        \n",
    "        preds = (output.squeeze() > best_thr).type(torch.IntTensor)\n",
    "        preds = np.squeeze(preds.cpu().numpy())\n",
    "        all_test_preds += list(preds.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "033dc0bfba3afa9b72e30c710fbdb3ca2911203e"
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({\n",
    "    'qid': test_df.qid,\n",
    "    'prediction': all_test_preds\n",
    "})\n",
    "\n",
    "# Make sure the columns are in the correct order\n",
    "sub = sub[['qid', 'prediction']]\n",
    "\n",
    "sub.to_csv('submission.csv', index=False, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
